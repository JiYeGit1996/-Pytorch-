{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\.conda\\envs\\torch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\user\\.conda\\envs\\torch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\user\\.conda\\envs\\torch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\user\\.conda\\envs\\torch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\user\\.conda\\envs\\torch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\user\\.conda\\envs\\torch\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n",
      "0.6.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = './data/FashionMNIST'\n",
    "    ,train = True\n",
    "    ,download = True\n",
    "    ,transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):#line 1\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()# line 3\n",
    "        #第一层的输入为1，参数依赖数据集（用的是单色图像）\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=6,kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6,out_channels=12,kernel_size=5)\n",
    "        \n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4,out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120,out_features=60) \n",
    "        self.out = nn.Linear(in_features=60,out_features=10)\n",
    "        #最后一层输出为10，分类有10个类别，也依赖数据集\n",
    "    def forward(self,t):  \n",
    "        #(1) input layer\n",
    "        t = t \n",
    "        \n",
    "        #(2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t,kernel_size=2,stride=2)\n",
    "        \n",
    "         #(3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t,kernel_size=2,stride=2)\n",
    "        \n",
    "        #(4)hidden liner layer\n",
    "        t = t.reshape(-1,12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        #(5)hidden liner layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        #(6) outpur layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t,dim=1)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting out with Tensorboard(Network Graph and Images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = SummaryWriter()\n",
    "\n",
    "network = Network()\n",
    "images,labels = next(iter(train_loader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "tb.add_image('images',grid)\n",
    "tb.add_graph(network,images)\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: 46630 loss: 351.0307514369488\n",
      "epoch: 1 total_correct: 51243 loss: 237.669718131423\n",
      "epoch: 2 total_correct: 51797 loss: 221.65378358960152\n",
      "epoch: 3 total_correct: 52117 loss: 213.49102199077606\n",
      "epoch: 4 total_correct: 52459 loss: 205.06129117310047\n",
      "epoch: 5 total_correct: 52434 loss: 204.5997589379549\n",
      "epoch: 6 total_correct: 52750 loss: 196.88377752900124\n",
      "epoch: 7 total_correct: 52767 loss: 195.12314468622208\n",
      "epoch: 8 total_correct: 52977 loss: 191.36762250959873\n",
      "epoch: 9 total_correct: 52926 loss: 189.84433709084988\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=100,shuffle=True)\n",
    "optimizer = optim.Adam(network.parameters(),lr=0.01)\n",
    "\n",
    "images,labels = next(iter(train_loader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "tb = SummaryWriter()\n",
    "tb.add_image('images',grid)\n",
    "tb.add_graph(network,images)\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    total_loss=0\n",
    "    total_correct=0\n",
    "\n",
    "    for batch in train_loader:#get batch\n",
    "        images,labels = batch\n",
    "\n",
    "        preds = network(images)#pass batch\n",
    "        loss = F.cross_entropy(preds,labels)#calculate loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()#calculate gradients\n",
    "        optimizer.step()#update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_correct += get_num_correct(preds,labels)\n",
    "    \n",
    "    tb.add_scalar('Loss',total_loss,epoch)\n",
    "    tb.add_scalar('Number Correct',total_correct,epoch)\n",
    "    tb.add_scalar('Accuracy',total_correct / len(train_set),epoch)\n",
    "    \n",
    "    #tb.add_histogram('conv1.bias',network.conv1.bias,epoch)\n",
    "    #tb.add_histogram('conv1.weight',network.conv1.weight,epoch)\n",
    "    #tb.add_histogram('conv1.weight.grad',network.conv1.weight.grad,epoch)\n",
    "    \n",
    "    for name,weight in network.named_parameters():\n",
    "        tb.add_histogram(name,weight,epoch)\n",
    "        tb.add_histogram(f'{name}.grad',weight.grad,epoch) \n",
    "\n",
    "    print(\"epoch:\",epoch,\"total_correct:\",total_correct,\"loss:\",total_loss)\n",
    "    \n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [100,1000,10000]\n",
    "lr_list = [.01,.001,.0001,.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_size in batch_size_list:\n",
    "    for lr in lr_list:\n",
    "\n",
    "        network = Network()\n",
    "        train_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=True)\n",
    "        optimizer = optim.Adam(network.parameters(),lr=lr)\n",
    "\n",
    "        images,labels = next(iter(train_loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "        comment = f'batch_size={batch_size} lr={lr}'\n",
    "        tb = SummaryWriter(comment=comment)\n",
    "        tb.add_image('images',grid)\n",
    "        tb.add_graph(network,images)\n",
    "\n",
    "        for epoch in range(10):\n",
    "\n",
    "            total_loss=0\n",
    "            total_correct=0\n",
    "\n",
    "            for batch in train_loader:#get batch\n",
    "                images,labels = batch\n",
    "\n",
    "                preds = network(images)#pass batch\n",
    "                loss = F.cross_entropy(preds,labels)#calculate loss\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()#calculate gradients\n",
    "                optimizer.step()#update weights\n",
    "\n",
    "                total_loss += loss.item()*batch_size\n",
    "                total_correct += get_num_correct(preds,labels)\n",
    "    \n",
    "            tb.add_scalar('Loss',total_loss,epoch)\n",
    "            tb.add_scalar('Number Correct',total_correct,epoch)\n",
    "            tb.add_scalar('Accuracy',total_correct / len(train_set),epoch)\n",
    "    \n",
    "    #tb.add_histogram('conv1.bias',network.conv1.bias,epoch)\n",
    "    #tb.add_histogram('conv1.weight',network.conv1.weight,epoch)\n",
    "    #tb.add_histogram('conv1.weight.grad',network.conv1.weight.grad,epoch)\n",
    "    \n",
    "            for name,weight in network.named_parameters():\n",
    "                tb.add_histogram(name,weight,epoch)\n",
    "                tb.add_histogram(f'{name}.grad',weight.grad,epoch) \n",
    "\n",
    "            print(\"epoch:\",epoch,\"total_correct:\",total_correct,\"loss:\",total_loss)\n",
    "    \n",
    "        tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    lr=[.01,.001]\n",
    "    ,batch_size=[10,100,1000]\n",
    "    ,shuffle=[True,False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.01, 0.001], [10, 100, 1000], [True, False]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_values = [v for v in parameters.values()]\n",
    "param_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 10 True\n",
      "0.01 10 False\n",
      "0.01 100 True\n",
      "0.01 100 False\n",
      "0.01 1000 True\n",
      "0.01 1000 False\n",
      "0.001 10 True\n",
      "0.001 10 False\n",
      "0.001 100 True\n",
      "0.001 100 False\n",
      "0.001 1000 True\n",
      "0.001 1000 False\n"
     ]
    }
   ],
   "source": [
    "for lr,batch_size,shuffle in product(*param_values):\n",
    "    print(lr,batch_size,shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 total_correct: 45060 loss: 39601.12988737412\n",
      "epoch: 1 total_correct: 48339 loss: 32289.857091507874\n",
      "epoch: 0 total_correct: 45814 loss: 37922.699721083045\n",
      "epoch: 1 total_correct: 48568 loss: 31723.53576716734\n",
      "epoch: 0 total_correct: 47386 loss: 33439.22770768404\n",
      "epoch: 1 total_correct: 51557 loss: 22848.083353042603\n",
      "epoch: 0 total_correct: 45985 loss: 36910.90911030769\n",
      "epoch: 1 total_correct: 51182 loss: 24044.046126306057\n",
      "epoch: 0 total_correct: 38412 loss: 57546.21088504791\n",
      "epoch: 1 total_correct: 47899 loss: 31707.999616861343\n",
      "epoch: 0 total_correct: 35092 loss: 65412.20438480377\n",
      "epoch: 1 total_correct: 45817 loss: 36716.35419130325\n",
      "epoch: 0 total_correct: 47462 loss: 33414.3913878873\n",
      "epoch: 1 total_correct: 51893 loss: 21822.1542068664\n",
      "epoch: 0 total_correct: 46979 loss: 34627.42826823611\n",
      "epoch: 1 total_correct: 51630 loss: 22692.014530757442\n",
      "epoch: 0 total_correct: 42561 loss: 46678.36547791958\n",
      "epoch: 1 total_correct: 48328 loss: 31308.32652449608\n",
      "epoch: 0 total_correct: 42151 loss: 46953.94967794418\n",
      "epoch: 1 total_correct: 48429 loss: 30718.373906612396\n",
      "epoch: 0 total_correct: 30964 loss: 86833.53674411774\n",
      "epoch: 1 total_correct: 43055 loss: 43599.04623031616\n",
      "epoch: 0 total_correct: 24921 loss: 100725.52436590195\n",
      "epoch: 1 total_correct: 41438 loss: 48805.050790309906\n"
     ]
    }
   ],
   "source": [
    "for lr,batch_size,shuffle in product(*param_values):\n",
    "\n",
    "    comment = f'batch_size={batch_size} lr={lr} shuffle={shuffle}'\n",
    "    network = Network()\n",
    "    train_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=shuffle)\n",
    "    optimizer = optim.Adam(network.parameters(),lr=lr)\n",
    "\n",
    "    images,labels = next(iter(train_loader))\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "    #comment = f'batch_size={batch_size} lr={lr}'\n",
    "    tb = SummaryWriter(comment=comment)\n",
    "    tb.add_image('images',grid)\n",
    "    tb.add_graph(network,images)\n",
    "\n",
    "    for epoch in range(2):\n",
    "\n",
    "        total_loss=0\n",
    "        total_correct=0\n",
    "\n",
    "        for batch in train_loader:#get batch\n",
    "            images,labels = batch\n",
    "\n",
    "            preds = network(images)#pass batch\n",
    "            loss = F.cross_entropy(preds,labels)#calculate loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()#calculate gradients\n",
    "            optimizer.step()#update weights\n",
    "\n",
    "            total_loss += loss.item()*batch_size\n",
    "            total_correct += get_num_correct(preds,labels)\n",
    "    \n",
    "        tb.add_scalar('Loss',total_loss,epoch)\n",
    "        tb.add_scalar('Number Correct',total_correct,epoch)\n",
    "        tb.add_scalar('Accuracy',total_correct / len(train_set),epoch)\n",
    "    \n",
    "    #tb.add_histogram('conv1.bias',network.conv1.bias,epoch)\n",
    "    #tb.add_histogram('conv1.weight',network.conv1.weight,epoch)\n",
    "    #tb.add_histogram('conv1.weight.grad',network.conv1.weight.grad,epoch)\n",
    "    \n",
    "        for name,weight in network.named_parameters():\n",
    "            tb.add_histogram(name,weight,epoch)\n",
    "            tb.add_histogram(f'{name}.grad',weight.grad,epoch) \n",
    "\n",
    "        print(\"epoch:\",epoch,\"total_correct:\",total_correct,\"loss:\",total_loss)\n",
    "    \n",
    "    tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-torch]",
   "language": "python",
   "name": "conda-env-.conda-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
